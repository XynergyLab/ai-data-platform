[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engines.
sql_alchemy_conn = mysql+pymysql://root:password@mysql:3306/airflow

# The encoding for the databases
sql_engine_encoding = utf-8

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool.
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool.
sql_alchemy_pool_recycle = 1800

# Whether to load examples or not
load_examples = False

# Secret key to save connection passwords in the db
fernet_key = UKMzEm3yIuFYEq1y3-2FxPNWSVwRASpahmQ9kQfEr-E=

# Whether to disable pickling dags
donot_pickle = False

# How long a DagRun should be up before timing out / failing
dagrun_timeout = 60

# Whether to hide paused DAGs by default
hide_paused_dags_by_default = False

# Number of workers to refresh DAGs
dag_file_processor_timeout = 50

# Number of seconds between data interval for cron schedules
interval_check = 15

# Whether to serve logs from the local file system
serve_logs = True

# Log format for handlers
log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# The maximum number of log folders to maintain
max_num_log_folders = 30

[webserver]
# The base url of your website as airflow cannot guess what domain or
# subdomain is being requested
base_url = http://localhost:8080

# The host interface on which to listen
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Number of seconds the webserver waits before restarting gunicorn
web_server_worker_timeout = 120

# The number of workers to use when running the Gunicorn web server
workers = 4

# The number of worker processes that Gunicorn should use
# to sync to the worker spool
worker_refresh_interval = 30

# Secret key used to run your flask app
secret_key = temporary_key

# Number of seconds the gunicorn webserver will wait before timing out
web_server_timeout = 120

# Expose the configuration file in the web server
expose_config = True

# Enable the experimental API
enable_experimental_api = True

# Default DAG view. Valid values are: tree, graph, duration, gantt, landing_times
dag_default_view = tree

# Default DAG orientation. Valid values are: LR (Left->Right), TB (Top->Bottom), 
# RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

# Enable authentication
authenticate = True

# Disable the public role so non-authenticated users no longer have access 
# to information
auth_backend = airflow.api.auth.backend.basic_auth

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.basic_auth

[operators]
# The default owner assigned to each new operator
default_owner = airflow

[elasticsearch]
elasticsearch_host =

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = smtp.example.com
smtp_starttls = True
smtp_ssl = False
smtp_user = ml-alerts
smtp_password = password
smtp_port = 587
smtp_mail_from = ml-alerts@example.com

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information).
# This defines how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# The number of task instances allowed to run concurrently
max_threads = 2

# Should the scheduler run tasks after the DAG has been paused
run_paused_dags = False

# Whether to enable health check
enable_health_check = True

[mesos]
# Mesos master address which MesosExecutor will connect to.
master = localhost:5050

# The framework name which Airflow scheduler will register itself as on mesos
framework_name = Airflow

[kubernetes]
# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker
worker_container_repository =
worker_container_tag =
worker_container_image_pull_policy = IfNotPresent

# If True, all worker pods will be deleted upon termination
delete_worker_pods = True

# The Kubernetes namespace where airflow workers should be created
namespace = default

# The name of the service account used by the workers for accessing resources
# in the Kubernetes cluster
service_account_name = airflow-worker

# Hard limit of worker pods
worker_pods_creation_batch_size = 1

[dask]
# This section only applies if you are using the DaskExecutor
cluster_address = 127.0.0.1:8786

[sensors]
# This affects sensor tasks such as BaseSensorOperator.
# While a sensor is sleeping, it is considered "running" by Airflow.
# This was used to prevent the scheduler from queuing more executions of
# the same sensor instance.
# Now, the max_active_runs configuration of the DAG limits the number that
# can be running, and if this is set to false,
# the sensor will get rescheduled anew when it gets deferred.
# Setting this to false enables single-instance execution,
# which is more efficient for most sensors because new code means new SQL queries.
keep_alive = True

[ml_integrations]
# Integration with MLflow
mlflow_tracking_uri = http://ml-mlflow:5000
mlflow_registry_uri = http://ml-mlflow:5000

# Integration with Triton Inference Server
triton_server_url = http://ml-triton-inference:8000
triton_management_url = http://ml-triton-inference:8001

# Vector database integration
milvus_host = ml-vector-milvus
milvus_port = 19530
qdrant_host = ml-vector-qdrant
qdrant_port = 6333

# Path to PyTorch distributed cluster
pytorch_master_host = ml-pytorch-master
pytorch_master_port = 29500

