# TorchServe Configuration File

# Server port configurations
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# Default model settings
default_workers_per_model=2
job_queue_size=100
model_store=/home/model-server/model-store
workflow_store=/home/model-server/model-store/workflows
model_snapshot={\
  "name": "startup.cfg",\
  "modelCount": 0,\
  "models": {}\
}

# Metrics collection settings for Prometheus
metrics_format=prometheus
metrics_enabled=true
metrics_mode=pull

# Performance tuning parameters
number_of_netty_threads=4
netty_client_threads=4
default_response_timeout=300
unregister_model_timeout=120
initial_worker_port=9000
async_logging=true
prefer_direct_buffer=true
max_request_size=6553500
max_response_size=6553500
number_of_gpu=0

# Custom model loading
service_envelope=kserve

# Logging configuration
vmargs=-Dlog4j.configurationFile=/home/model-server/config/log4j2.xml
enable_envvars_config=true

# CORS settings
cors_allowed_origin=*
cors_allowed_methods=GET, POST, PUT, DELETE, OPTIONS
cors_allowed_headers=X-Requested-With, Content-Type, Accept, Origin

# Default model loading on startup
# model_id.1=model_name1
# model_id.2=model_name2

# Default worker parallelism settings (adjust based on hardware)
cpu_worker_affinity=true
job_queue_sampling_policy=RoundRobin

# Tensorflow model configurations
allow_load_tensorflow=true
ipc_socket_binding=bind_and_connect

